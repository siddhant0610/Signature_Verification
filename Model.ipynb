{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd05b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4789bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_to_signature(gray, thresh_val=220):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
    "    coords = cv2.findNonZero(th)\n",
    "    if coords is None:\n",
    "        return gray\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    return gray[y:y+h, x:x+w]\n",
    "\n",
    "def _deskew(gray):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    if coords.size == 0:\n",
    "        return gray\n",
    "    rect = cv2.minAreaRect(coords.astype(np.float32))\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    if abs(angle) < 1:\n",
    "        return gray\n",
    "    (h, w) = gray.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(gray, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_signature(\n",
    "    img_input,\n",
    "    size=224,\n",
    "    mean=0.5,\n",
    "    std=0.5,\n",
    "    deskew=True,\n",
    "    crop=True,\n",
    "    as_tensor=False\n",
    "):\n",
    "    # 1. Load image as grayscale\n",
    "    if isinstance(img_input, str):\n",
    "        gray = cv2.imread(img_input, cv2.IMREAD_GRAYSCALE)\n",
    "        if gray is None:\n",
    "            raise ValueError(f\"Could not read image from path: {img_input}\")\n",
    "    else:\n",
    "        img = img_input\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        elif len(img.shape) == 3 and img.shape[2] == 1:\n",
    "            gray = img[:, :, 0]\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    if crop:\n",
    "        gray = _crop_to_signature(gray)\n",
    "\n",
    "    if deskew:\n",
    "        gray = _deskew(gray)\n",
    "\n",
    "    h, w = gray.shape\n",
    "    if h == 0 or w == 0:\n",
    "        raise ValueError(\"Empty image encountered after cropping/deskewing.\")\n",
    "\n",
    "    scale = size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    h2, w2 = resized.shape\n",
    "    pad_top = (size - h2) // 2\n",
    "    pad_bottom = size - h2 - pad_top\n",
    "    pad_left = (size - w2) // 2\n",
    "    pad_right = size - w2 - pad_left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=255\n",
    "    )\n",
    "\n",
    "    img_float = padded.astype(np.float32) / 255.0\n",
    "    img_norm = (img_float - mean) / std\n",
    "\n",
    "    if as_tensor:\n",
    "        img_norm = np.expand_dims(img_norm, axis=0)  # (1,H,W)\n",
    "\n",
    "    return img_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392e6d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9eeb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models,Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c294947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, ff_dim, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim\n",
    "        )\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "\n",
    "        self.ffn = Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(num_heads * key_dim)\n",
    "        ])\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn = self.mha(x, x, x, training=training)\n",
    "        attn = self.dropout1(attn, training=training)\n",
    "        x = self.norm1(x + attn)\n",
    "\n",
    "        ffn = self.ffn(x)\n",
    "        ffn = self.dropout2(ffn, training=training)\n",
    "        return self.norm2(x + ffn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07b7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_transformer_embedding_model(\n",
    "    input_shape=(128, 128, 1),\n",
    "    embedding_dim=128,\n",
    "    conv_filters=(32, 64, 128),\n",
    "    transformer_layers=1,\n",
    "    num_heads=4,\n",
    "    key_dim=32,\n",
    "    ff_dim=256,\n",
    "    dropout=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    CNN + Transformer embedding backbone\n",
    "    Output: L2-normalized embedding\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape, name=\"signature_input\")\n",
    "    x = inputs\n",
    "\n",
    "    # ----- CNN Backbone -----\n",
    "    for f in conv_filters:\n",
    "        x = layers.Conv2D(f, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPool2D((2,2))(x)\n",
    "\n",
    "    # x shape: (B, H', W', C)\n",
    "    _, h, w, c = x.shape\n",
    "    seq_len = h * w\n",
    "\n",
    "    # ----- Flatten to tokens -----\n",
    "    x = layers.Reshape((seq_len, c))(x)\n",
    "\n",
    "    token_dim = num_heads * key_dim\n",
    "    if c != token_dim:\n",
    "        x = layers.Dense(token_dim, name=\"token_projection\")(x)\n",
    "\n",
    "    # ----- Transformer Encoder(s) -----\n",
    "    for i in range(transformer_layers):\n",
    "        x = TransformerEncoder(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout=dropout,\n",
    "            name=f\"transformer_encoder_{i}\"\n",
    "        )(x)\n",
    "\n",
    "    # ----- Pool tokens -----\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # ----- Embedding Head -----\n",
    "    x = layers.Dense(embedding_dim, use_bias=False, name=\"embedding_dense\")(x)\n",
    "    x = layers.BatchNormalization(name=\"embedding_bn\")(x)\n",
    "    x = layers.Lambda(\n",
    "        lambda t: tf.math.l2_normalize(t, axis=1),\n",
    "        name=\"l2_normalized_embedding\"\n",
    "    )(x)\n",
    "\n",
    "    return models.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=x,\n",
    "        name=\"cnn_transformer_embedding_model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf10b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(layers.Layer):\n",
    "    def __init__(self, num_classes, s=30.0, m=0.5, easy_margin=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(embedding_dim, self.num_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings, labels = inputs\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        cos_theta = tf.matmul(embeddings, W)\n",
    "        cos_theta = tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        sin_theta = tf.sqrt(1.0 - tf.square(cos_theta))\n",
    "        cos_theta_m = cos_theta * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            cos_theta_m = tf.where(cos_theta > 0, cos_theta_m, cos_theta)\n",
    "        else:\n",
    "            cos_theta_m = tf.where(\n",
    "                cos_theta > self.th,\n",
    "                cos_theta_m,\n",
    "                cos_theta - self.mm\n",
    "            )\n",
    "\n",
    "        one_hot = tf.one_hot(labels, depth=self.num_classes)\n",
    "        logits = self.s * (one_hot * cos_theta_m + (1.0 - one_hot) * cos_theta)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeded527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_transformer_arcface_training_model(\n",
    "    num_classes,\n",
    "    input_shape=(128, 128, 1),\n",
    "    embedding_dim=128\n",
    "):\n",
    "    embedding_model = build_cnn_transformer_embedding_model(\n",
    "        input_shape=input_shape,\n",
    "        embedding_dim=embedding_dim,\n",
    "        transformer_layers=1,     # start with 1\n",
    "        num_heads=4,\n",
    "        key_dim=32,\n",
    "        ff_dim=256\n",
    "    )\n",
    "\n",
    "    image_input = layers.Input(shape=input_shape, name=\"image_input\")\n",
    "    label_input = layers.Input(shape=(), dtype=tf.int32, name=\"label_input\")\n",
    "\n",
    "    embeddings = embedding_model(image_input)\n",
    "\n",
    "    logits = ArcMarginProduct(\n",
    "        num_classes=num_classes,\n",
    "        s=30.0,\n",
    "        m=0.5\n",
    "    )([embeddings, label_input])\n",
    "\n",
    "    training_model = models.Model(\n",
    "        inputs={\"image_input\": image_input, \"label_input\": label_input},\n",
    "        outputs=logits,\n",
    "        name=\"cnn_transformer_arcface_signature_model\"\n",
    "    )\n",
    "\n",
    "    training_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return training_model, embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "481776a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1320\n",
      "Unique signers (classes): 55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "full_org_dir = \"signatures/full_org\"   # ğŸ”´ change to your path\n",
    "\n",
    "# get all images (png/jpg â€“ adjust if needed)\n",
    "image_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "\n",
    "def parse_signer_id_from_filename(path):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      'original_1_1.png' -> signer 0\n",
    "      'original_2_3.png' -> signer 1\n",
    "    Pattern assumed: <prefix>_<signerId>_<sampleId>.png\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(path)       # 'original_1_1.png'\n",
    "    parts = fname.split(\"_\")             # ['original', '1', '1.png']\n",
    "    signer_str = parts[1]                # '1'\n",
    "    signer_int = int(signer_str) - 1     # make 0-based: 0,1,2,...\n",
    "    return signer_int\n",
    "\n",
    "labels = [parse_signer_id_from_filename(p) for p in image_paths]\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print(\"Total images:\", len(image_paths))\n",
    "print(\"Unique signers (classes):\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e6ce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists â†’ dataset\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(p):\n",
    "        p = p.decode()\n",
    "        img = preprocess_signature(p,size=128,as_tensor=True)\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    img = tf.numpy_function(_preprocess,[path],tf.float32)\n",
    "    img.set_shape((128,128,1))\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    return {\"image_input\":img,\"label_input\":label}, label\n",
    "\n",
    "train_ds = (path_ds\n",
    "            .shuffle(len(image_paths))\n",
    "            .map(load_and_preprocess,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(8)                          # small batch for CPU\n",
    "            .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9d51a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_batch.shape: (8, 128, 128, 1)\n",
      "label_batch.shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Corrected mapping function ----\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(path_bytes):\n",
    "        # tf.string -> python str\n",
    "        path_str = path_bytes.decode(\"utf-8\")\n",
    "\n",
    "        # call your existing preprocess_signature\n",
    "        # it returns (1, H, W) when as_tensor=True in your implementation\n",
    "        img = preprocess_signature(\n",
    "            img_input=path_str,\n",
    "            size=128,\n",
    "            mean=0.5,\n",
    "            std=0.5,\n",
    "            deskew=True,\n",
    "            crop=True,\n",
    "            as_tensor=True  # returns shape (1, H, W)\n",
    "        )\n",
    "\n",
    "        # Ensure shape: convert (1, H, W) -> (H, W, 1)\n",
    "        # sometimes preprocess returns (H,W) if as_tensor=False; handle both\n",
    "        img = np.asarray(img)\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            # (1, H, W) -> (H, W, 1)\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "        elif img.ndim == 2:\n",
    "            # (H, W) -> (H, W, 1)\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        elif img.ndim == 3 and img.shape[-1] != 1:\n",
    "            # e.g., (H, W, C) leave as is\n",
    "            pass\n",
    "\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    # Wrap the numpy function for tf.data\n",
    "    img = tf.numpy_function(_preprocess, [path], tf.float32)\n",
    "\n",
    "    # IMPORTANT: set the static shape to (128,128,1)\n",
    "    img.set_shape((128, 128, 1))\n",
    "\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    # return inputs dict (as your model expects) and the target label\n",
    "    return {\"image_input\": img, \"label_input\": label}, label\n",
    "\n",
    "# ---- Recreate the dataset (use your existing image_paths, labels) ----\n",
    "batch_size = 8\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "train_ds = (\n",
    "    path_ds\n",
    "    .shuffle(buffer_size=len(image_paths))\n",
    "    .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ---- Quick debug: fetch one batch and print shapes ----\n",
    "for batch_inputs, batch_labels in train_ds.take(1):\n",
    "    img_batch = batch_inputs[\"image_input\"]\n",
    "    print(\"img_batch.shape:\", img_batch.shape)   # should be (batch_size, 128, 128, 1)\n",
    "    print(\"label_batch.shape:\", batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58337cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kashi\\OneDrive\\Desktop\\GitHub_ML\\Sig\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_transformer_arcface_signature_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cnn_transformer_arcface_signature_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_transformer_emâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">242,944</span> â”‚ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_product  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,040</span> â”‚ cnn_transformer_â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ArcMarginProduct</span>)  â”‚                   â”‚            â”‚ label_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m1\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cnn_transformer_emâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m242,944\u001b[0m â”‚ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (\u001b[38;5;45mNone\u001b[0m)            â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_product  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)        â”‚      \u001b[38;5;34m7,040\u001b[0m â”‚ cnn_transformer_â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mArcMarginProduct\u001b[0m)  â”‚                   â”‚            â”‚ label_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,984</span> (976.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m249,984\u001b[0m (976.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,280</span> (973.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m249,280\u001b[0m (973.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 426ms/step - accuracy: 0.0000e+00 - loss: 17.9478\n",
      "Epoch 2/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 397ms/step - accuracy: 0.0000e+00 - loss: 16.0224\n",
      "Epoch 3/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 438ms/step - accuracy: 0.0045 - loss: 13.7180\n",
      "Epoch 4/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 484ms/step - accuracy: 0.0341 - loss: 11.2827\n",
      "Epoch 5/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 488ms/step - accuracy: 0.0856 - loss: 9.0788\n",
      "Epoch 6/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 312ms/step - accuracy: 0.1727 - loss: 7.0118\n",
      "Epoch 7/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 233ms/step - accuracy: 0.2811 - loss: 5.2635\n",
      "Epoch 8/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 221ms/step - accuracy: 0.3462 - loss: 4.5537\n",
      "Epoch 9/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 226ms/step - accuracy: 0.4250 - loss: 3.4481\n",
      "Epoch 10/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 2s/step - accuracy: 0.5371 - loss: 2.5934\n",
      "Epoch 11/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 245ms/step - accuracy: 0.5674 - loss: 2.1888\n",
      "Epoch 12/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 229ms/step - accuracy: 0.6303 - loss: 1.8901\n",
      "Epoch 13/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 226ms/step - accuracy: 0.6856 - loss: 1.3599\n",
      "Epoch 14/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 235ms/step - accuracy: 0.6917 - loss: 1.3136\n",
      "Epoch 15/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 232ms/step - accuracy: 0.7121 - loss: 1.2375\n",
      "Epoch 16/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 230ms/step - accuracy: 0.7614 - loss: 0.9215\n",
      "Epoch 17/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 253ms/step - accuracy: 0.7970 - loss: 0.7930\n",
      "Epoch 18/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 224ms/step - accuracy: 0.8235 - loss: 0.6790\n",
      "Epoch 19/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 6s/step - accuracy: 0.7735 - loss: 0.9004\n",
      "Epoch 20/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 644ms/step - accuracy: 0.8189 - loss: 0.6874\n",
      "Epoch 21/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 2s/step - accuracy: 0.8674 - loss: 0.4912\n",
      "Epoch 22/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 245ms/step - accuracy: 0.8409 - loss: 0.5487\n",
      "Epoch 23/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 240ms/step - accuracy: 0.8583 - loss: 0.5311\n",
      "Epoch 24/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 231ms/step - accuracy: 0.8386 - loss: 0.6160\n",
      "Epoch 25/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 239ms/step - accuracy: 0.8727 - loss: 0.4614\n",
      "Epoch 26/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 397ms/step - accuracy: 0.8977 - loss: 0.3481\n",
      "Epoch 27/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 500ms/step - accuracy: 0.8606 - loss: 0.4957\n",
      "Epoch 28/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 484ms/step - accuracy: 0.8500 - loss: 0.5653\n",
      "Epoch 29/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 491ms/step - accuracy: 0.8795 - loss: 0.4373\n",
      "Epoch 30/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 483ms/step - accuracy: 0.8947 - loss: 0.3536\n",
      "Epoch 31/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 552ms/step - accuracy: 0.8955 - loss: 0.3789\n"
     ]
    }
   ],
   "source": [
    "num_classes = 55\n",
    "input_shape = (128,128,1)\n",
    "\n",
    "training_model, embedding_model = build_cnn_transformer_arcface_training_model(\n",
    "    num_classes=num_classes,\n",
    "    input_shape=input_shape,\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "training_model.summary()\n",
    "\n",
    "history = training_model.fit(\n",
    "    train_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87043661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. show model input info\n",
    "# print(\"MODEL INPUTS:\")\n",
    "# for i, inp in enumerate(training_model.inputs):\n",
    "#     print(i, inp.name, inp.shape, inp.dtype)\n",
    "\n",
    "# print(\"\\nMODEL SUMMARY:\")\n",
    "# training_model.summary()\n",
    "\n",
    "# # 2. peek one element from dataset (not batched)\n",
    "# # one = next(iter(path_ds))   # path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "# # print(\"\\nExample path,label (raw):\", one)\n",
    "\n",
    "# # # 3. fetch one batch from train_ds and print shapes/dtypes/values\n",
    "# # for batch_inputs, batch_labels in train_ds.take(1):\n",
    "# #     # If dataset yields input dict:\n",
    "# #     if isinstance(batch_inputs, dict):\n",
    "# #         img_batch = batch_inputs.get(\"image_input\") or batch_inputs.get(\"image\")\n",
    "# #         label_in = batch_inputs.get(\"label_input\") or batch_inputs.get(\"label\")\n",
    "# #     else:\n",
    "# #         # if it's a list/tuple of inputs, try ordering\n",
    "# #         if isinstance(batch_inputs, (list, tuple)):\n",
    "# #             img_batch = batch_inputs[0]\n",
    "# #             label_in = batch_inputs[1] if len(batch_inputs)>1 else None\n",
    "# #         else:\n",
    "# #             print(\"Unknown batch_inputs type:\", type(batch_inputs))\n",
    "# #             img_batch = None\n",
    "# #             label_in = None\n",
    "\n",
    "# #     print(\"\\nBATCH IMG shape:\", None if img_batch is None else img_batch.shape, \"dtype:\", None if img_batch is None else img_batch.dtype)\n",
    "# #     print(\"BATCH LABELS shape:\", batch_labels.shape, \"dtype:\", batch_labels.dtype)\n",
    "# #     # check label values range\n",
    "# #     lb = batch_labels.numpy()\n",
    "# #     print(\"label min, max:\", lb.min(), lb.max())\n",
    "# #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97aba573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_embedding_from_model(image_path, embedding_model, size=128):\n",
    "    # uses your preprocess_signature(img_path, size=..., as_tensor=True)\n",
    "    img = preprocess_signature(img_input=image_path, size=size, as_tensor=True)\n",
    "    img = np.asarray(img)\n",
    "    # handle shape (1,H,W) -> (H,W,1)\n",
    "    if img.ndim == 3 and img.shape[0] == 1:\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "    elif img.ndim == 2:\n",
    "        img = np.expand_dims(img, -1)\n",
    "    X = np.expand_dims(img, axis=0).astype(np.float32)  # (1,H,W,1)\n",
    "    emb = embedding_model.predict(X, verbose=0)[0]\n",
    "    emb = emb / (np.linalg.norm(emb) + 1e-12)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4d6fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_MEANS.shape: (55, 128) DB_USER_IDS.shape: (55,)\n"
     ]
    }
   ],
   "source": [
    "def sid_from_filename(path):\n",
    "    # filenames like original_1_1.png -> signer id is middle token\n",
    "    return int(os.path.basename(path).split(\"_\")[1]) - 1\n",
    "\n",
    "def build_db_means_from_folder(full_org_dir, embedding_model, size=128, batch_size=32):\n",
    "    paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "    by_user = {}\n",
    "    for p in paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        emb = get_embedding_from_model(p, embedding_model, size=size)\n",
    "        by_user.setdefault(sid, []).append(emb)\n",
    "\n",
    "    user_ids = []\n",
    "    means = []\n",
    "    for uid in sorted(by_user.keys()):\n",
    "        embs = np.vstack(by_user[uid])\n",
    "        mean = embs.mean(axis=0)\n",
    "        mean = mean / (np.linalg.norm(mean) + 1e-12)\n",
    "        user_ids.append(uid)\n",
    "        means.append(mean.astype(np.float32))\n",
    "\n",
    "    means = np.vstack(means)  # (N_users, D)\n",
    "    user_ids = np.array(user_ids, dtype=np.int32)\n",
    "    return means, user_ids\n",
    "\n",
    "# Example call (adjust path)\n",
    "DB_MEANS, DB_USER_IDS = build_db_means_from_folder(\"signatures/full_org\", embedding_model, size=128)\n",
    "print(\"DB_MEANS.shape:\", DB_MEANS.shape, \"DB_USER_IDS.shape:\", DB_USER_IDS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dac9d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "def save_embeddings_npz(npz_path, means, user_ids):\n",
    "    np.savez_compressed(npz_path, means=means.astype(np.float32), user_ids=user_ids.astype(np.int32))\n",
    "    print(\"Saved:\", npz_path)\n",
    "\n",
    "def load_embeddings_npz(npz_path):\n",
    "    z = np.load(npz_path)\n",
    "    means = z[\"means\"].astype(np.float32)\n",
    "    user_ids = z[\"user_ids\"].astype(np.int32)\n",
    "    # ensure normalized\n",
    "    means = means / (np.linalg.norm(means, axis=1, keepdims=True) + 1e-12)\n",
    "    return means, user_ids\n",
    "\n",
    "# Save:\n",
    "save_embeddings_npz(\"embeddings.npz\", DB_MEANS, DB_USER_IDS)\n",
    "\n",
    "# Later load:\n",
    "# DB_MEANS, DB_USER_IDS = load_embeddings_npz(\"embeddings.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79ba060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def batch_get_embeddings(paths, embedding_model, size=128, batch_size=16):\n",
    "    embs = []\n",
    "    batch = []\n",
    "    for p in paths:\n",
    "        img = preprocess_signature(img_input=p, size=size, as_tensor=True)\n",
    "        img = np.asarray(img)\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "        elif img.ndim == 2:\n",
    "            img = np.expand_dims(img, -1)\n",
    "        batch.append(img.astype(np.float32))\n",
    "        if len(batch) >= batch_size:\n",
    "            X = np.stack(batch, axis=0)\n",
    "            e = embedding_model.predict(X, verbose=0)\n",
    "            embs.append(e)\n",
    "            batch=[]\n",
    "    if batch:\n",
    "        X = np.stack(batch, axis=0)\n",
    "        e = embedding_model.predict(X, verbose=0)\n",
    "        embs.append(e)\n",
    "    if len(embs)==0:\n",
    "        return np.zeros((0, embedding_model.output_shape[-1]), dtype=np.float32)\n",
    "    embs = np.vstack(embs)\n",
    "    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12)\n",
    "    return embs\n",
    "\n",
    "def compute_eer_threshold(full_org_dir, full_forg_dir, embedding_model, refs_per_user=3, size=128):\n",
    "    # group paths\n",
    "    org_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "    forg_paths = sorted(glob.glob(os.path.join(full_forg_dir, \"*.png\")))\n",
    "    org_by_sid = {}\n",
    "    forg_by_sid = {}\n",
    "    for p in org_paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        org_by_sid.setdefault(sid, []).append(p)\n",
    "    for p in forg_paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        forg_by_sid.setdefault(sid, []).append(p)\n",
    "\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    for sid, paths in org_by_sid.items():\n",
    "        if len(paths) < refs_per_user + 1:\n",
    "            continue\n",
    "        refs = paths[:refs_per_user]\n",
    "        ref_embs = batch_get_embeddings(refs, embedding_model, size=size)\n",
    "        ref_mean = np.mean(ref_embs, axis=0)\n",
    "        ref_mean = ref_mean / (np.linalg.norm(ref_mean) + 1e-12)\n",
    "\n",
    "        for test_p in paths[refs_per_user:]:\n",
    "            emb = batch_get_embeddings([test_p], embedding_model, size=size)[0]\n",
    "            score = float(np.dot(emb, ref_mean))\n",
    "            y_scores.append(score); y_true.append(1)\n",
    "        for f in forg_by_sid.get(sid, []):\n",
    "            emb = batch_get_embeddings([f], embedding_model, size=size)[0]\n",
    "            score = float(np.dot(emb, ref_mean))\n",
    "            y_scores.append(score); y_true.append(0)\n",
    "\n",
    "    if len(set(y_true)) < 2:\n",
    "        raise RuntimeError(\"Not enough genuine/forged data to compute EER.\")\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer_thr = thresholds[eer_idx]\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2.0\n",
    "    print(f\"AUC={auc:.4f}, EER={eer:.4f}, EER_threshold={eer_thr:.4f}\")\n",
    "    return eer_thr, {\"auc\":auc, \"eer\":eer}\n",
    "\n",
    "# Example (may take some time on CPU)\n",
    "# thr, metrics = compute_eer_threshold(\"signatures/full_org\", \"signatures/full_forg\", embedding_model, refs_per_user=3, size=128)\n",
    "# print(\"Use threshold:\", thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "194116f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_original_or_forged(image_path, embedding_model, db_means, db_user_ids, threshold=0.75, size=128):\n",
    "    query_emb = get_embedding_from_model(image_path, embedding_model, size=size)\n",
    "    sims = db_means @ query_emb   # vectorized cosine (means should be normalized)\n",
    "    best_idx = int(np.argmax(sims))\n",
    "    best_score = float(sims[best_idx])\n",
    "    matched_user = int(db_user_ids[best_idx])\n",
    "    if best_score >= threshold:\n",
    "        return {\"result\":\"ORIGINAL\", \"matched_user_id\": matched_user, \"score\": best_score}\n",
    "    else:\n",
    "        return {\"result\":\"FORGED\", \"matched_user_id\": None, \"score\": best_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d71eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase layer and distane formula change \n",
    "def predict_with_margin(\n",
    "    image_path,\n",
    "    embedding_model,\n",
    "    db_means,\n",
    "    db_user_ids,\n",
    "    sim_threshold=0.78,\n",
    "    margin_threshold=0.05,\n",
    "    size=128\n",
    "):\n",
    "    emb = get_embedding_from_model(image_path, embedding_model, size=size)\n",
    "    sims = db_means @ emb\n",
    "\n",
    "    sorted_idx = np.argsort(sims)[::-1]\n",
    "    best = sims[sorted_idx[0]]\n",
    "    second_best = sims[sorted_idx[1]]\n",
    "\n",
    "    if best >= sim_threshold and (best - second_best) >= margin_threshold:\n",
    "        return {\n",
    "            \"result\": \"ORIGINAL\",\n",
    "            \"user\": int(db_user_ids[sorted_idx[0]]),\n",
    "            \"best_score\": float(best),\n",
    "            \"margin\": float(best - second_best)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"result\": \"FORGED\",\n",
    "            \"best_score\": float(best),\n",
    "            \"margin\": float(best - second_best)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f31135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'ORIGINAL', 'user': 12, 'best_score': 0.9499318599700928, 'margin': 0.31710588932037354}\n",
      "{'result': 'FORGED', 'best_score': 0.7444943189620972, 'margin': 0.06659668684005737}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'we can use  sim_threshold=0.88-0.95,    \\n    margin_threshold=0.04-0.08  '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved DB if needed\n",
    "DB_MEANS, DB_USER_IDS = load_embeddings_npz(\"embeddings.npz\")\n",
    "\n",
    "# compute threshold once (recommended)\n",
    "# thr, _ = compute_eer_threshold(\"signatures/full_org\", \"signatures/full_forg\", embedding_model)\n",
    "\n",
    "# quick test\n",
    "res1 = predict_with_margin(\n",
    "    \"signatures/full_org/original_13_1.png\",\n",
    "    embedding_model,\n",
    "    DB_MEANS,\n",
    "    DB_USER_IDS,\n",
    "    sim_threshold=0.89,      # âœ… correct name\n",
    "    margin_threshold=0.07\n",
    ")\n",
    "\n",
    "res2 = predict_with_margin(\n",
    "    \"signatures/full_forg/forgeries_13_1.png\",\n",
    "    embedding_model,\n",
    "    DB_MEANS,\n",
    "    DB_USER_IDS,\n",
    "    sim_threshold=0.88,      # âœ… forgery name\n",
    "    margin_threshold=0.07\n",
    ")\n",
    "\n",
    "print(res1)\n",
    "print(res2)\n",
    "\n",
    "'''we can use  sim_threshold=0.88-0.95,    \n",
    "    margin_threshold=0.04-0.08  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8bf12a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kashi\\onedrive\\desktop\\github_ml\\sig\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
