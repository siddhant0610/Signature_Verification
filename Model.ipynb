{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd05b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4789bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_to_signature(gray, thresh_val=220):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
    "    coords = cv2.findNonZero(th)\n",
    "    if coords is None:\n",
    "        return gray\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    return gray[y:y+h, x:x+w]\n",
    "\n",
    "def _deskew(gray):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    if coords.size == 0:\n",
    "        return gray\n",
    "    rect = cv2.minAreaRect(coords.astype(np.float32))\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    if abs(angle) < 1:\n",
    "        return gray\n",
    "    (h, w) = gray.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(gray, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_signature(\n",
    "    img_input,\n",
    "    size=224,\n",
    "    mean=0.5,\n",
    "    std=0.5,\n",
    "    deskew=True,\n",
    "    crop=True,\n",
    "    as_tensor=False\n",
    "):\n",
    "    # 1. Load image as grayscale\n",
    "    if isinstance(img_input, str):\n",
    "        gray = cv2.imread(img_input, cv2.IMREAD_GRAYSCALE)\n",
    "        if gray is None:\n",
    "            raise ValueError(f\"Could not read image from path: {img_input}\")\n",
    "    else:\n",
    "        img = img_input\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        elif len(img.shape) == 3 and img.shape[2] == 1:\n",
    "            gray = img[:, :, 0]\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    if crop:\n",
    "        gray = _crop_to_signature(gray)\n",
    "\n",
    "    if deskew:\n",
    "        gray = _deskew(gray)\n",
    "\n",
    "    h, w = gray.shape\n",
    "    if h == 0 or w == 0:\n",
    "        raise ValueError(\"Empty image encountered after cropping/deskewing.\")\n",
    "\n",
    "    scale = size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    h2, w2 = resized.shape\n",
    "    pad_top = (size - h2) // 2\n",
    "    pad_bottom = size - h2 - pad_top\n",
    "    pad_left = (size - w2) // 2\n",
    "    pad_right = size - w2 - pad_left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=255\n",
    "    )\n",
    "\n",
    "    img_float = padded.astype(np.float32) / 255.0\n",
    "    img_norm = (img_float - mean) / std\n",
    "\n",
    "    if as_tensor:\n",
    "        img_norm = np.expand_dims(img_norm, axis=0)  # (1,H,W)\n",
    "\n",
    "    return img_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392e6d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9eeb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1. Embedding Backbone\n",
    "# =========================\n",
    "def build_embedding_model(input_shape=(224, 224, 1), embedding_dim=512, train_backbone=True):\n",
    "    \"\"\"\n",
    "    Builds an image -> embedding model.\n",
    "    - Grayscale input\n",
    "    - Converts to RGB\n",
    "    - Uses ResNet50 backbone + Dense + BN + L2 norm\n",
    "    \"\"\"\n",
    "    # Grayscale input\n",
    "    inputs = layers.Input(shape=input_shape, name=\"signature_input\")\n",
    "\n",
    "    # Convert 1-channel grayscale to 3 channels (RGB-like)\n",
    "    x_rgb = layers.Lambda(lambda t: tf.image.grayscale_to_rgb(t),\n",
    "                          name=\"gray_to_rgb\")(inputs)\n",
    "    # Now x_rgb shape: (H, W, 3)\n",
    "\n",
    "    # Standard ResNet50 backbone (NO input_tensor!)\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",  # you can set to None while debugging\n",
    "        input_shape=(input_shape[0], input_shape[1], 3)\n",
    "    )\n",
    "    base_model.trainable = train_backbone\n",
    "\n",
    "    x = base_model(x_rgb, training=train_backbone)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "\n",
    "    # Embedding head: Dense + BatchNorm + L2 normalization\n",
    "    x = layers.Dense(embedding_dim, use_bias=False, name=\"embedding_dense\")(x)\n",
    "    x = layers.BatchNormalization(name=\"embedding_bn\")(x)\n",
    "\n",
    "    # ‚ùó IMPORTANT: wrap l2_normalize in a Lambda layer\n",
    "    x = layers.Lambda(\n",
    "        lambda t: tf.nn.l2_normalize(t, axis=1),\n",
    "        name=\"l2_normalized_embedding\"\n",
    "    )(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=x, name=\"signature_embedding_model\")\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 2. ArcFace layer\n",
    "# =========================\n",
    "\n",
    "class ArcMarginProduct(layers.Layer):\n",
    "    def __init__(self, num_classes, s=30.0, m=0.5, easy_margin=False, **kwargs):\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(embedding_dim, self.num_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings, labels = inputs\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        cos_theta = tf.matmul(embeddings, W)\n",
    "        cos_theta = tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        sin_theta = tf.sqrt(1.0 - tf.square(cos_theta))\n",
    "        cos_theta_m = cos_theta * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            cond = tf.cast(tf.greater(cos_theta, 0), tf.float32)\n",
    "            cos_theta_m = cond * cos_theta_m + (1.0 - cond) * cos_theta\n",
    "        else:\n",
    "            cond_v = cos_theta - self.th\n",
    "            cond = tf.cast(tf.nn.relu(cond_v), tf.bool)\n",
    "            keep = tf.where(cond, cos_theta_m, cos_theta - self.mm)\n",
    "            cos_theta_m = keep\n",
    "\n",
    "        labels_one_hot = tf.one_hot(labels, depth=self.num_classes)\n",
    "        logits = self.s * (labels_one_hot * cos_theta_m + (1.0 - labels_one_hot) * cos_theta)\n",
    "        return logits\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"s\": self.s,\n",
    "            \"m\": self.m,\n",
    "            \"easy_margin\": self.easy_margin\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "\n",
    "def build_arcface_training_model(num_classes, input_shape=(224, 224, 1), embedding_dim=512):\n",
    "    embedding_model = build_embedding_model(\n",
    "        input_shape=input_shape,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "\n",
    "    image_input = layers.Input(shape=input_shape, name=\"image_input\")\n",
    "    label_input = layers.Input(shape=(), name=\"label_input\", dtype=tf.int32)\n",
    "\n",
    "    embeddings = embedding_model(image_input)\n",
    "    logits = ArcMarginProduct(num_classes=num_classes, s=30.0, m=0.5)(\n",
    "        [embeddings, label_input]\n",
    "    )\n",
    "\n",
    "    training_model = models.Model(\n",
    "        inputs=[image_input, label_input],\n",
    "        outputs=logits,\n",
    "        name=\"arcface_signature_model\"\n",
    "    )\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    training_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return training_model, embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58337cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100   # must match your actual number of unique signer IDs\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "training_model, embedding_model = build_arcface_training_model(\n",
    "    num_classes=num_classes,\n",
    "    input_shape=input_shape,\n",
    "    embedding_dim=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481776a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1320\n",
      "Unique signers (classes): 55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "full_org_dir = \"signatures/full_org\"   # üî¥ change to your path\n",
    "\n",
    "# get all images (png/jpg ‚Äì adjust if needed)\n",
    "image_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "\n",
    "def parse_signer_id_from_filename(path):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      'original_1_1.png' -> signer 0\n",
    "      'original_2_3.png' -> signer 1\n",
    "    Pattern assumed: <prefix>_<signerId>_<sampleId>.png\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(path)       # 'original_1_1.png'\n",
    "    parts = fname.split(\"_\")             # ['original', '1', '1.png']\n",
    "    signer_str = parts[1]                # '1'\n",
    "    signer_int = int(signer_str) - 1     # make 0-based: 0,1,2,...\n",
    "    return signer_int\n",
    "\n",
    "labels = [parse_signer_id_from_filename(p) for p in image_paths]\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print(\"Total images:\", len(image_paths))\n",
    "print(\"Unique signers (classes):\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6ce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(path_bytes):\n",
    "        path_str = path_bytes.decode(\"utf-8\")\n",
    "        img = preprocess_signature(\n",
    "            img_input=path_str,\n",
    "            size=224,\n",
    "            mean=0.5,\n",
    "            std=0.5,\n",
    "            deskew=True,\n",
    "            crop=True,\n",
    "            as_tensor=True,\n",
    "        )\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            img = np.transpose(img, (1, 2, 0))  # (H,W,1)\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    img = tf.numpy_function(_preprocess, [path], tf.float32)\n",
    "    img.set_shape((224, 224, 1))\n",
    "\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    return {\"image_input\": img, \"label_input\": label}, label\n",
    "\n",
    "train_ds = (\n",
    "    path_ds\n",
    "    .shuffle(len(image_paths))\n",
    "    .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 6/42\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m44:59\u001b[0m 75s/step - accuracy: 0.0000e+00 - loss: 19.2532"
     ]
    }
   ],
   "source": [
    "training_model.fit(train_ds, epochs=5,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63215d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess your image to shape (1, 224, 224, 1) float32\n",
    "# x = preprocess_signature(...)\n",
    "\n",
    "embedding = embedding_model.predict(x)  # shape: (1, 512), already L2-normalized\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
