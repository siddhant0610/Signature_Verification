{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd05b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4789bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_to_signature(gray, thresh_val=220):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
    "    coords = cv2.findNonZero(th)\n",
    "    if coords is None:\n",
    "        return gray\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    return gray[y:y+h, x:x+w]\n",
    "\n",
    "def _deskew(gray):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    if coords.size == 0:\n",
    "        return gray\n",
    "    rect = cv2.minAreaRect(coords.astype(np.float32))\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    if abs(angle) < 1:\n",
    "        return gray\n",
    "    (h, w) = gray.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(gray, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_signature(\n",
    "    img_input,\n",
    "    size=224,\n",
    "    mean=0.5,\n",
    "    std=0.5,\n",
    "    deskew=True,\n",
    "    crop=True,\n",
    "    as_tensor=False\n",
    "):\n",
    "    # 1. Load image as grayscale\n",
    "    if isinstance(img_input, str):\n",
    "        gray = cv2.imread(img_input, cv2.IMREAD_GRAYSCALE)\n",
    "        if gray is None:\n",
    "            raise ValueError(f\"Could not read image from path: {img_input}\")\n",
    "    else:\n",
    "        img = img_input\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        elif len(img.shape) == 3 and img.shape[2] == 1:\n",
    "            gray = img[:, :, 0]\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    if crop:\n",
    "        gray = _crop_to_signature(gray)\n",
    "\n",
    "    if deskew:\n",
    "        gray = _deskew(gray)\n",
    "\n",
    "    h, w = gray.shape\n",
    "    if h == 0 or w == 0:\n",
    "        raise ValueError(\"Empty image encountered after cropping/deskewing.\")\n",
    "\n",
    "    scale = size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    h2, w2 = resized.shape\n",
    "    pad_top = (size - h2) // 2\n",
    "    pad_bottom = size - h2 - pad_top\n",
    "    pad_left = (size - w2) // 2\n",
    "    pad_right = size - w2 - pad_left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=255\n",
    "    )\n",
    "\n",
    "    img_float = padded.astype(np.float32) / 255.0\n",
    "    img_norm = (img_float - mean) / std\n",
    "\n",
    "    if as_tensor:\n",
    "        img_norm = np.expand_dims(img_norm, axis=0)  # (1,H,W)\n",
    "\n",
    "    return img_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392e6d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9eeb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1. Embedding Backbone\n",
    "# =========================\n",
    "def build_light_embedding_model(input_shape=(128, 128, 1), embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Lightweight CNN: much smaller than ResNet50, good for CPU training.\n",
    "    Input:  (H,W,1)\n",
    "    Output: L2-normalized embedding (embedding_dim,)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name=\"signature_input\")\n",
    "\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 64x64\n",
    "\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 32x32\n",
    "\n",
    "    x = layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 16x16\n",
    "\n",
    "    x = layers.Conv2D(256, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # 256-d\n",
    "\n",
    "    x = layers.Dense(embedding_dim, use_bias=False, name=\"embedding_dense\")(x)\n",
    "    x = layers.BatchNormalization(name=\"embedding_bn\")(x)\n",
    "    x = layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1),\n",
    "                      name=\"l2_normalized_embedding\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=x, name=\"light_embedding_model\")\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 2. ArcFace layer\n",
    "# =========================\n",
    "\n",
    "\n",
    "class ArcMarginProduct(layers.Layer):\n",
    "    def __init__(self, num_classes, s=30.0, m=0.5, easy_margin=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(embedding_dim, self.num_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings, labels = inputs\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        cos_theta = tf.matmul(embeddings, W)\n",
    "        cos_theta = tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        sin_theta = tf.sqrt(1.0 - tf.square(cos_theta))\n",
    "        cos_theta_m = cos_theta * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            cond = tf.cast(tf.greater(cos_theta, 0), tf.float32)\n",
    "            cos_theta_m = cond * cos_theta_m + (1.0 - cond) * cos_theta\n",
    "        else:\n",
    "            cond_v = cos_theta - self.th\n",
    "            cond = tf.cast(tf.nn.relu(cond_v), tf.bool)\n",
    "            cos_theta_m = tf.where(cond, cos_theta_m, cos_theta - self.mm)\n",
    "\n",
    "        labels_one_hot = tf.one_hot(labels, depth=self.num_classes)\n",
    "        logits = self.s * (labels_one_hot * cos_theta_m + (1.0 - labels_one_hot) * cos_theta)\n",
    "        return logits\n",
    "\n",
    "def build_light_arcface_training_model(num_classes,\n",
    "                                       input_shape=(128, 128, 1),\n",
    "                                       embedding_dim=128):\n",
    "    embedding_model = build_light_embedding_model(\n",
    "        input_shape=input_shape,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "\n",
    "    image_input = layers.Input(shape=input_shape, name=\"image_input\")\n",
    "    label_input = layers.Input(shape=(), name=\"label_input\", dtype=tf.int32)\n",
    "\n",
    "    embeddings = embedding_model(image_input)\n",
    "    logits = ArcMarginProduct(num_classes=num_classes, s=30.0, m=0.5)(\n",
    "        [embeddings, label_input]\n",
    "    )\n",
    "\n",
    "    training_model = models.Model(\n",
    "        inputs={\"image_input\": image_input, \"label_input\": label_input},\n",
    "        outputs=logits,\n",
    "        name=\"light_arcface_signature_model\"\n",
    "    )\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    training_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),  # slightly higher LR for small net\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return training_model, embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "481776a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1320\n",
      "Unique signers (classes): 55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "full_org_dir = \"signatures/full_org\"   # ğŸ”´ change to your path\n",
    "\n",
    "# get all images (png/jpg â€“ adjust if needed)\n",
    "image_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "\n",
    "def parse_signer_id_from_filename(path):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      'original_1_1.png' -> signer 0\n",
    "      'original_2_3.png' -> signer 1\n",
    "    Pattern assumed: <prefix>_<signerId>_<sampleId>.png\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(path)       # 'original_1_1.png'\n",
    "    parts = fname.split(\"_\")             # ['original', '1', '1.png']\n",
    "    signer_str = parts[1]                # '1'\n",
    "    signer_int = int(signer_str) - 1     # make 0-based: 0,1,2,...\n",
    "    return signer_int\n",
    "\n",
    "labels = [parse_signer_id_from_filename(p) for p in image_paths]\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print(\"Total images:\", len(image_paths))\n",
    "print(\"Unique signers (classes):\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e6ce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists â†’ dataset\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(p):\n",
    "        p = p.decode()\n",
    "        img = preprocess_signature(p,size=128,as_tensor=True)\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    img = tf.numpy_function(_preprocess,[path],tf.float32)\n",
    "    img.set_shape((128,128,1))\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    return {\"image_input\":img,\"label_input\":label}, label\n",
    "\n",
    "train_ds = (path_ds\n",
    "            .shuffle(len(image_paths))\n",
    "            .map(load_and_preprocess,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(8)                          # small batch for CPU\n",
    "            .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9d51a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_batch.shape: (8, 128, 128, 1)\n",
      "label_batch.shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Corrected mapping function ----\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(path_bytes):\n",
    "        # tf.string -> python str\n",
    "        path_str = path_bytes.decode(\"utf-8\")\n",
    "\n",
    "        # call your existing preprocess_signature\n",
    "        # it returns (1, H, W) when as_tensor=True in your implementation\n",
    "        img = preprocess_signature(\n",
    "            img_input=path_str,\n",
    "            size=128,\n",
    "            mean=0.5,\n",
    "            std=0.5,\n",
    "            deskew=True,\n",
    "            crop=True,\n",
    "            as_tensor=True  # returns shape (1, H, W)\n",
    "        )\n",
    "\n",
    "        # Ensure shape: convert (1, H, W) -> (H, W, 1)\n",
    "        # sometimes preprocess returns (H,W) if as_tensor=False; handle both\n",
    "        img = np.asarray(img)\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            # (1, H, W) -> (H, W, 1)\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "        elif img.ndim == 2:\n",
    "            # (H, W) -> (H, W, 1)\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        elif img.ndim == 3 and img.shape[-1] != 1:\n",
    "            # e.g., (H, W, C) leave as is\n",
    "            pass\n",
    "\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    # Wrap the numpy function for tf.data\n",
    "    img = tf.numpy_function(_preprocess, [path], tf.float32)\n",
    "\n",
    "    # IMPORTANT: set the static shape to (128,128,1)\n",
    "    img.set_shape((128, 128, 1))\n",
    "\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    # return inputs dict (as your model expects) and the target label\n",
    "    return {\"image_input\": img, \"label_input\": label}, label\n",
    "\n",
    "# ---- Recreate the dataset (use your existing image_paths, labels) ----\n",
    "batch_size = 8\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "train_ds = (\n",
    "    path_ds\n",
    "    .shuffle(buffer_size=len(image_paths))\n",
    "    .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ---- Quick debug: fetch one batch and print shapes ----\n",
    "for batch_inputs, batch_labels in train_ds.take(1):\n",
    "    img_batch = batch_inputs[\"image_input\"]\n",
    "    print(\"img_batch.shape:\", img_batch.shape)   # should be (batch_size, 128, 128, 1)\n",
    "    print(\"label_batch.shape:\", batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58337cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 177ms/step - accuracy: 0.0000e+00 - loss: 18.3725\n",
      "Epoch 2/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 116ms/step - accuracy: 7.5758e-04 - loss: 17.5829\n",
      "Epoch 3/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.0038 - loss: 16.9511\n",
      "Epoch 4/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.0098 - loss: 16.2301\n",
      "Epoch 5/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.0098 - loss: 15.4141\n",
      "Epoch 6/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.0136 - loss: 14.3824\n",
      "Epoch 7/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.0424 - loss: 13.2087\n",
      "Epoch 8/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.0606 - loss: 12.3664\n",
      "Epoch 9/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - accuracy: 0.0955 - loss: 10.8589\n",
      "Epoch 10/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.1288 - loss: 10.1295\n",
      "Epoch 11/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 118ms/step - accuracy: 0.1561 - loss: 9.2312\n",
      "Epoch 12/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 5s/step - accuracy: 0.2045 - loss: 8.1822\n",
      "Epoch 13/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 198ms/step - accuracy: 0.2197 - loss: 7.6827\n",
      "Epoch 14/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 206ms/step - accuracy: 0.2697 - loss: 6.9524\n",
      "Epoch 15/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 251ms/step - accuracy: 0.3280 - loss: 6.2140\n",
      "Epoch 16/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 118ms/step - accuracy: 0.3288 - loss: 5.8727\n",
      "Epoch 17/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.3773 - loss: 5.5149\n",
      "Epoch 18/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.3871 - loss: 5.2046\n",
      "Epoch 19/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.4083 - loss: 4.7575\n",
      "Epoch 20/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.4439 - loss: 4.3100\n",
      "Epoch 21/50\n",
      "\u001b[1m 50/165\u001b[0m \u001b[32mâ”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m13s\u001b[0m 118ms/step - accuracy: 0.4767 - loss: 3.8053"
     ]
    }
   ],
   "source": [
    "num_classes = 55  # must match your actual number of unique signer IDs\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "training_model, embedding_model = build_light_arcface_training_model(\n",
    "    num_classes=num_classes,\n",
    "    input_shape=(128, 128, 1),\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "history = training_model.fit(train_ds, epochs=50)  # start with few epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0585c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INPUTS:\n",
      "0 image_input (None, 128, 128, 1) float32\n",
      "1 label_input (None,) int32\n",
      "\n",
      "MODEL SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"light_arcface_signature_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"light_arcface_signature_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ light_embedding_moâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">421,120</span> â”‚ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_productâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,040</span> â”‚ light_embedding_â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ArcMarginProduct</span>)  â”‚                   â”‚            â”‚ label_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m1\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ light_embedding_moâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m421,120\u001b[0m â”‚ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (\u001b[38;5;45mNone\u001b[0m)            â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_productâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)        â”‚      \u001b[38;5;34m7,040\u001b[0m â”‚ light_embedding_â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mArcMarginProduct\u001b[0m)  â”‚                   â”‚            â”‚ label_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,283,970</span> (4.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,283,970\u001b[0m (4.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">427,904</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m427,904\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">855,810</span> (3.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m855,810\u001b[0m (3.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. show model input info\n",
    "print(\"MODEL INPUTS:\")\n",
    "for i, inp in enumerate(training_model.inputs):\n",
    "    print(i, inp.name, inp.shape, inp.dtype)\n",
    "\n",
    "print(\"\\nMODEL SUMMARY:\")\n",
    "training_model.summary()\n",
    "\n",
    "# 2. peek one element from dataset (not batched)\n",
    "# one = next(iter(path_ds))   # path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "# print(\"\\nExample path,label (raw):\", one)\n",
    "\n",
    "# # 3. fetch one batch from train_ds and print shapes/dtypes/values\n",
    "# for batch_inputs, batch_labels in train_ds.take(1):\n",
    "#     # If dataset yields input dict:\n",
    "#     if isinstance(batch_inputs, dict):\n",
    "#         img_batch = batch_inputs.get(\"image_input\") or batch_inputs.get(\"image\")\n",
    "#         label_in = batch_inputs.get(\"label_input\") or batch_inputs.get(\"label\")\n",
    "#     else:\n",
    "#         # if it's a list/tuple of inputs, try ordering\n",
    "#         if isinstance(batch_inputs, (list, tuple)):\n",
    "#             img_batch = batch_inputs[0]\n",
    "#             label_in = batch_inputs[1] if len(batch_inputs)>1 else None\n",
    "#         else:\n",
    "#             print(\"Unknown batch_inputs type:\", type(batch_inputs))\n",
    "#             img_batch = None\n",
    "#             label_in = None\n",
    "\n",
    "#     print(\"\\nBATCH IMG shape:\", None if img_batch is None else img_batch.shape, \"dtype:\", None if img_batch is None else img_batch.dtype)\n",
    "#     print(\"BATCH LABELS shape:\", batch_labels.shape, \"dtype:\", batch_labels.dtype)\n",
    "#     # check label values range\n",
    "#     lb = batch_labels.numpy()\n",
    "#     print(\"label min, max:\", lb.min(), lb.max())\n",
    "#     break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
