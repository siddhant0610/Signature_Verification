{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4789bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_to_signature(gray, thresh_val=220):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
    "    coords = cv2.findNonZero(th)\n",
    "    if coords is None:\n",
    "        return gray\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    return gray[y:y+h, x:x+w]\n",
    "\n",
    "def _deskew(gray):\n",
    "    # ... your code ...\n",
    "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    coords = np.column_stack(np.where(th > 0))\n",
    "    if coords.size == 0:\n",
    "        return gray\n",
    "    rect = cv2.minAreaRect(coords.astype(np.float32))\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    if abs(angle) < 1:\n",
    "        return gray\n",
    "    (h, w) = gray.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(gray, M, (w, h),\n",
    "                             flags=cv2.INTER_CUBIC,\n",
    "                             borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_signature(\n",
    "    img_input,\n",
    "    size=224,\n",
    "    mean=0.5,\n",
    "    std=0.5,\n",
    "    deskew=True,\n",
    "    crop=True,\n",
    "    as_tensor=False\n",
    "):\n",
    "    # 1. Load image as grayscale\n",
    "    if isinstance(img_input, str):\n",
    "        gray = cv2.imread(img_input, cv2.IMREAD_GRAYSCALE)\n",
    "        if gray is None:\n",
    "            raise ValueError(f\"Could not read image from path: {img_input}\")\n",
    "    else:\n",
    "        img = img_input\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        elif len(img.shape) == 3 and img.shape[2] == 1:\n",
    "            gray = img[:, :, 0]\n",
    "        else:\n",
    "            gray = img.copy()\n",
    "\n",
    "    gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    if crop:\n",
    "        gray = _crop_to_signature(gray)\n",
    "\n",
    "    if deskew:\n",
    "        gray = _deskew(gray)\n",
    "\n",
    "    h, w = gray.shape\n",
    "    if h == 0 or w == 0:\n",
    "        raise ValueError(\"Empty image encountered after cropping/deskewing.\")\n",
    "\n",
    "    scale = size / max(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    resized = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    h2, w2 = resized.shape\n",
    "    pad_top = (size - h2) // 2\n",
    "    pad_bottom = size - h2 - pad_top\n",
    "    pad_left = (size - w2) // 2\n",
    "    pad_right = size - w2 - pad_left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=255\n",
    "    )\n",
    "\n",
    "    img_float = padded.astype(np.float32) / 255.0\n",
    "    img_norm = (img_float - mean) / std\n",
    "\n",
    "    if as_tensor:\n",
    "        img_norm = np.expand_dims(img_norm, axis=0)  # (1,H,W)\n",
    "\n",
    "    return img_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392e6d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9eeb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 1. Embedding Backbone\n",
    "# =========================\n",
    "def build_light_embedding_model(input_shape=(128, 128, 1), embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Lightweight CNN: much smaller than ResNet50, good for CPU training.\n",
    "    Input:  (H,W,1)\n",
    "    Output: L2-normalized embedding (embedding_dim,)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name=\"signature_input\")\n",
    "\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 64x64\n",
    "\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 32x32\n",
    "\n",
    "    x = layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)   # 16x16\n",
    "\n",
    "    x = layers.Conv2D(256, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # 256-d\n",
    "\n",
    "    x = layers.Dense(embedding_dim, use_bias=False, name=\"embedding_dense\")(x)\n",
    "    x = layers.BatchNormalization(name=\"embedding_bn\")(x)\n",
    "    x = layers.Lambda(lambda t: tf.nn.l2_normalize(t, axis=1),\n",
    "                      name=\"l2_normalized_embedding\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=x, name=\"light_embedding_model\")\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 2. ArcFace layer\n",
    "# =========================\n",
    "\n",
    "\n",
    "class ArcMarginProduct(layers.Layer):\n",
    "    def __init__(self, num_classes, s=30.0, m=0.5, easy_margin=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embedding_dim = input_shape[0][-1]\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(embedding_dim, self.num_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings, labels = inputs\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        cos_theta = tf.matmul(embeddings, W)\n",
    "        cos_theta = tf.clip_by_value(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        sin_theta = tf.sqrt(1.0 - tf.square(cos_theta))\n",
    "        cos_theta_m = cos_theta * self.cos_m - sin_theta * self.sin_m\n",
    "\n",
    "        if self.easy_margin:\n",
    "            cond = tf.cast(tf.greater(cos_theta, 0), tf.float32)\n",
    "            cos_theta_m = cond * cos_theta_m + (1.0 - cond) * cos_theta\n",
    "        else:\n",
    "            cond_v = cos_theta - self.th\n",
    "            cond = tf.cast(tf.nn.relu(cond_v), tf.bool)\n",
    "            cos_theta_m = tf.where(cond, cos_theta_m, cos_theta - self.mm)\n",
    "\n",
    "        labels_one_hot = tf.one_hot(labels, depth=self.num_classes)\n",
    "        logits = self.s * (labels_one_hot * cos_theta_m + (1.0 - labels_one_hot) * cos_theta)\n",
    "        return logits\n",
    "\n",
    "def build_light_arcface_training_model(num_classes,\n",
    "                                       input_shape=(128, 128, 1),\n",
    "                                       embedding_dim=128):\n",
    "    embedding_model = build_light_embedding_model(\n",
    "        input_shape=input_shape,\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "\n",
    "    image_input = layers.Input(shape=input_shape, name=\"image_input\")\n",
    "    label_input = layers.Input(shape=(), name=\"label_input\", dtype=tf.int32)\n",
    "\n",
    "    embeddings = embedding_model(image_input)\n",
    "    logits = ArcMarginProduct(num_classes=num_classes, s=30.0, m=0.5)(\n",
    "        [embeddings, label_input]\n",
    "    )\n",
    "\n",
    "    training_model = models.Model(\n",
    "        inputs={\"image_input\": image_input, \"label_input\": label_input},\n",
    "        outputs=logits,\n",
    "        name=\"light_arcface_signature_model\"\n",
    "    )\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    training_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),  # slightly higher LR for small net\n",
    "        loss=loss_fn,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return training_model, embedding_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481776a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1320\n",
      "Unique signers (classes): 55\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "full_org_dir = \"signatures/full_org\"   # ğŸ”´ change to your path\n",
    "\n",
    "# get all images (png/jpg â€“ adjust if needed)\n",
    "image_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "\n",
    "def parse_signer_id_from_filename(path):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "      'original_1_1.png' -> signer 0\n",
    "      'original_2_3.png' -> signer 1\n",
    "    Pattern assumed: <prefix>_<signerId>_<sampleId>.png\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(path)       # 'original_1_1.png'\n",
    "    parts = fname.split(\"_\")             # ['original', '1', '1.png']\n",
    "    signer_str = parts[1]                # '1'\n",
    "    signer_int = int(signer_str) - 1     # make 0-based: 0,1,2,...\n",
    "    return signer_int\n",
    "\n",
    "labels = [parse_signer_id_from_filename(p) for p in image_paths]\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print(\"Total images:\", len(image_paths))\n",
    "print(\"Unique signers (classes):\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6ce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists â†’ dataset\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(p):\n",
    "        p = p.decode()\n",
    "        img = preprocess_signature(p,size=128,as_tensor=True)\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    img = tf.numpy_function(_preprocess,[path],tf.float32)\n",
    "    img.set_shape((128,128,1))\n",
    "    label = tf.cast(label,tf.int32)\n",
    "\n",
    "    return {\"image_input\":img,\"label_input\":label}, label\n",
    "\n",
    "train_ds = (path_ds\n",
    "            .shuffle(len(image_paths))\n",
    "            .map(load_and_preprocess,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(8)                          # small batch for CPU\n",
    "            .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d51a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_batch.shape: (8, 128, 128, 1)\n",
      "label_batch.shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Corrected mapping function ----\n",
    "def load_and_preprocess(path, label):\n",
    "    def _preprocess(path_bytes):\n",
    "        # tf.string -> python str\n",
    "        path_str = path_bytes.decode(\"utf-8\")\n",
    "\n",
    "        # call your existing preprocess_signature\n",
    "        # it returns (1, H, W) when as_tensor=True in your implementation\n",
    "        img = preprocess_signature(\n",
    "            img_input=path_str,\n",
    "            size=128,\n",
    "            mean=0.5,\n",
    "            std=0.5,\n",
    "            deskew=True,\n",
    "            crop=True,\n",
    "            as_tensor=True  # returns shape (1, H, W)\n",
    "        )\n",
    "\n",
    "        # Ensure shape: convert (1, H, W) -> (H, W, 1)\n",
    "        # sometimes preprocess returns (H,W) if as_tensor=False; handle both\n",
    "        img = np.asarray(img)\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            # (1, H, W) -> (H, W, 1)\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "        elif img.ndim == 2:\n",
    "            # (H, W) -> (H, W, 1)\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        elif img.ndim == 3 and img.shape[-1] != 1:\n",
    "            # e.g., (H, W, C) leave as is\n",
    "            pass\n",
    "\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "    # Wrap the numpy function for tf.data\n",
    "    img = tf.numpy_function(_preprocess, [path], tf.float32)\n",
    "\n",
    "    # IMPORTANT: set the static shape to (128,128,1)\n",
    "    img.set_shape((128, 128, 1))\n",
    "\n",
    "    label = tf.cast(label, tf.int32)\n",
    "\n",
    "    # return inputs dict (as your model expects) and the target label\n",
    "    return {\"image_input\": img, \"label_input\": label}, label\n",
    "\n",
    "# ---- Recreate the dataset (use your existing image_paths, labels) ----\n",
    "batch_size = 8\n",
    "\n",
    "path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "train_ds = (\n",
    "    path_ds\n",
    "    .shuffle(buffer_size=len(image_paths))\n",
    "    .map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ---- Quick debug: fetch one batch and print shapes ----\n",
    "for batch_inputs, batch_labels in train_ds.take(1):\n",
    "    img_batch = batch_inputs[\"image_input\"]\n",
    "    print(\"img_batch.shape:\", img_batch.shape)   # should be (batch_size, 128, 128, 1)\n",
    "    print(\"label_batch.shape:\", batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58337cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kashi\\OneDrive\\Desktop\\GitHub_ML\\Sig\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 161ms/step - accuracy: 0.0000e+00 - loss: 18.3815\n",
      "Epoch 2/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 235ms/step - accuracy: 7.5758e-04 - loss: 17.6750\n",
      "Epoch 3/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 233ms/step - accuracy: 0.0000e+00 - loss: 17.1322\n",
      "Epoch 4/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 198ms/step - accuracy: 0.0023 - loss: 16.5446\n",
      "Epoch 5/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 234ms/step - accuracy: 0.0174 - loss: 15.5031\n",
      "Epoch 6/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 320ms/step - accuracy: 0.0227 - loss: 14.5976\n",
      "Epoch 7/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 243ms/step - accuracy: 0.0424 - loss: 13.3456\n",
      "Epoch 8/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 251ms/step - accuracy: 0.0530 - loss: 12.1601\n",
      "Epoch 9/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 242ms/step - accuracy: 0.0992 - loss: 10.6600\n",
      "Epoch 10/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 159ms/step - accuracy: 0.1273 - loss: 9.6105\n",
      "Epoch 11/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 196ms/step - accuracy: 0.1856 - loss: 8.4619\n",
      "Epoch 12/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 239ms/step - accuracy: 0.2273 - loss: 7.7791\n",
      "Epoch 13/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 167ms/step - accuracy: 0.2811 - loss: 7.0453\n",
      "Epoch 14/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.3250 - loss: 6.2424\n",
      "Epoch 15/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.3553 - loss: 5.6123\n",
      "Epoch 16/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.4114 - loss: 5.0636\n",
      "Epoch 17/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.4152 - loss: 4.8853\n",
      "Epoch 18/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.4576 - loss: 4.4839\n",
      "Epoch 19/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.4697 - loss: 4.1499\n",
      "Epoch 20/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.4803 - loss: 3.9203\n",
      "Epoch 21/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.5205 - loss: 3.5908\n",
      "Epoch 22/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.5561 - loss: 3.3120\n",
      "Epoch 23/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 200ms/step - accuracy: 0.5773 - loss: 3.0063\n",
      "Epoch 24/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 237ms/step - accuracy: 0.6000 - loss: 2.7595\n",
      "Epoch 25/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 150ms/step - accuracy: 0.6379 - loss: 2.4352\n",
      "Epoch 26/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.6030 - loss: 2.4409\n",
      "Epoch 27/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.6462 - loss: 2.1039\n",
      "Epoch 28/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 112ms/step - accuracy: 0.6470 - loss: 2.0994\n",
      "Epoch 29/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 158ms/step - accuracy: 0.6894 - loss: 1.8391\n",
      "Epoch 30/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 257ms/step - accuracy: 0.7159 - loss: 1.5990\n",
      "Epoch 31/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 271ms/step - accuracy: 0.7326 - loss: 1.5453\n",
      "Epoch 32/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 197ms/step - accuracy: 0.7386 - loss: 1.4518\n",
      "Epoch 33/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 132ms/step - accuracy: 0.7689 - loss: 1.2418\n",
      "Epoch 34/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 116ms/step - accuracy: 0.7833 - loss: 1.1030\n",
      "Epoch 35/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.8023 - loss: 0.9989\n",
      "Epoch 36/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 112ms/step - accuracy: 0.7909 - loss: 1.0081\n",
      "Epoch 37/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.8189 - loss: 0.9138\n",
      "Epoch 38/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.7902 - loss: 1.0003\n",
      "Epoch 39/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 113ms/step - accuracy: 0.8530 - loss: 0.6905\n",
      "Epoch 40/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - accuracy: 0.8333 - loss: 0.7604\n",
      "Epoch 41/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.8614 - loss: 0.5833\n",
      "Epoch 42/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 192ms/step - accuracy: 0.8795 - loss: 0.5064\n",
      "Epoch 43/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m577s\u001b[0m 3s/step - accuracy: 0.8667 - loss: 0.5205\n",
      "Epoch 44/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 201ms/step - accuracy: 0.8697 - loss: 0.5311\n",
      "Epoch 45/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 153ms/step - accuracy: 0.8917 - loss: 0.3963\n",
      "Epoch 46/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - accuracy: 0.8962 - loss: 0.3777\n",
      "Epoch 47/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 115ms/step - accuracy: 0.9083 - loss: 0.3260\n",
      "Epoch 48/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 135ms/step - accuracy: 0.8985 - loss: 0.3931\n",
      "Epoch 49/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 119ms/step - accuracy: 0.9121 - loss: 0.3379\n",
      "Epoch 50/50\n",
      "\u001b[1m165/165\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 122ms/step - accuracy: 0.9121 - loss: 0.3154\n"
     ]
    }
   ],
   "source": [
    "num_classes = 55  # must match your actual number of unique signer IDs\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "training_model, embedding_model = build_light_arcface_training_model(\n",
    "    num_classes=num_classes,\n",
    "    input_shape=(128, 128, 1),\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "history = training_model.fit(train_ds, epochs=50)  # start with few epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87043661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INPUTS:\n",
      "0 image_input (None, 128, 128, 1) float32\n",
      "1 label_input (None,) int32\n",
      "\n",
      "MODEL SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"light_arcface_signature_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"light_arcface_signature_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ light_embedding_moâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">421,120</span> â”‚ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_product  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,040</span> â”‚ light_embedding_â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ArcMarginProduct</span>)  â”‚                   â”‚            â”‚ label_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ image_input         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m1\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ light_embedding_moâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m421,120\u001b[0m â”‚ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ label_input         â”‚ (\u001b[38;5;45mNone\u001b[0m)            â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ arc_margin_product  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)        â”‚      \u001b[38;5;34m7,040\u001b[0m â”‚ light_embedding_â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mArcMarginProduct\u001b[0m)  â”‚                   â”‚            â”‚ label_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,283,970</span> (4.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,283,970\u001b[0m (4.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">427,904</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m427,904\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">855,810</span> (3.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m855,810\u001b[0m (3.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. show model input info\n",
    "print(\"MODEL INPUTS:\")\n",
    "for i, inp in enumerate(training_model.inputs):\n",
    "    print(i, inp.name, inp.shape, inp.dtype)\n",
    "\n",
    "print(\"\\nMODEL SUMMARY:\")\n",
    "training_model.summary()\n",
    "\n",
    "# 2. peek one element from dataset (not batched)\n",
    "# one = next(iter(path_ds))   # path_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "# print(\"\\nExample path,label (raw):\", one)\n",
    "\n",
    "# # 3. fetch one batch from train_ds and print shapes/dtypes/values\n",
    "# for batch_inputs, batch_labels in train_ds.take(1):\n",
    "#     # If dataset yields input dict:\n",
    "#     if isinstance(batch_inputs, dict):\n",
    "#         img_batch = batch_inputs.get(\"image_input\") or batch_inputs.get(\"image\")\n",
    "#         label_in = batch_inputs.get(\"label_input\") or batch_inputs.get(\"label\")\n",
    "#     else:\n",
    "#         # if it's a list/tuple of inputs, try ordering\n",
    "#         if isinstance(batch_inputs, (list, tuple)):\n",
    "#             img_batch = batch_inputs[0]\n",
    "#             label_in = batch_inputs[1] if len(batch_inputs)>1 else None\n",
    "#         else:\n",
    "#             print(\"Unknown batch_inputs type:\", type(batch_inputs))\n",
    "#             img_batch = None\n",
    "#             label_in = None\n",
    "\n",
    "#     print(\"\\nBATCH IMG shape:\", None if img_batch is None else img_batch.shape, \"dtype:\", None if img_batch is None else img_batch.dtype)\n",
    "#     print(\"BATCH LABELS shape:\", batch_labels.shape, \"dtype:\", batch_labels.dtype)\n",
    "#     # check label values range\n",
    "#     lb = batch_labels.numpy()\n",
    "#     print(\"label min, max:\", lb.min(), lb.max())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97aba573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_embedding_from_model(image_path, embedding_model, size=128):\n",
    "    # uses your preprocess_signature(img_path, size=..., as_tensor=True)\n",
    "    img = preprocess_signature(img_input=image_path, size=size, as_tensor=True)\n",
    "    img = np.asarray(img)\n",
    "    # handle shape (1,H,W) -> (H,W,1)\n",
    "    if img.ndim == 3 and img.shape[0] == 1:\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "    elif img.ndim == 2:\n",
    "        img = np.expand_dims(img, -1)\n",
    "    X = np.expand_dims(img, axis=0).astype(np.float32)  # (1,H,W,1)\n",
    "    emb = embedding_model.predict(X, verbose=0)[0]\n",
    "    emb = emb / (np.linalg.norm(emb) + 1e-12)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d6fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_MEANS.shape: (55, 128) DB_USER_IDS.shape: (55,)\n"
     ]
    }
   ],
   "source": [
    "def sid_from_filename(path):\n",
    "    # filenames like original_1_1.png -> signer id is middle token\n",
    "    return int(os.path.basename(path).split(\"_\")[1]) - 1\n",
    "\n",
    "def build_db_means_from_folder(full_org_dir, embedding_model, size=128, batch_size=32):\n",
    "    paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "    by_user = {}\n",
    "    for p in paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        emb = get_embedding_from_model(p, embedding_model, size=size)\n",
    "        by_user.setdefault(sid, []).append(emb)\n",
    "\n",
    "    user_ids = []\n",
    "    means = []\n",
    "    for uid in sorted(by_user.keys()):\n",
    "        embs = np.vstack(by_user[uid])\n",
    "        mean = embs.mean(axis=0)\n",
    "        mean = mean / (np.linalg.norm(mean) + 1e-12)\n",
    "        user_ids.append(uid)\n",
    "        means.append(mean.astype(np.float32))\n",
    "\n",
    "    means = np.vstack(means)  # (N_users, D)\n",
    "    user_ids = np.array(user_ids, dtype=np.int32)\n",
    "    return means, user_ids\n",
    "\n",
    "# Example call (adjust path)\n",
    "DB_MEANS, DB_USER_IDS = build_db_means_from_folder(\"signatures/full_org\", embedding_model, size=128)\n",
    "print(\"DB_MEANS.shape:\", DB_MEANS.shape, \"DB_USER_IDS.shape:\", DB_USER_IDS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac9d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "def save_embeddings_npz(npz_path, means, user_ids):\n",
    "    np.savez_compressed(npz_path, means=means.astype(np.float32), user_ids=user_ids.astype(np.int32))\n",
    "    print(\"Saved:\", npz_path)\n",
    "\n",
    "def load_embeddings_npz(npz_path):\n",
    "    z = np.load(npz_path)\n",
    "    means = z[\"means\"].astype(np.float32)\n",
    "    user_ids = z[\"user_ids\"].astype(np.int32)\n",
    "    # ensure normalized\n",
    "    means = means / (np.linalg.norm(means, axis=1, keepdims=True) + 1e-12)\n",
    "    return means, user_ids\n",
    "\n",
    "# Save:\n",
    "save_embeddings_npz(\"embeddings.npz\", DB_MEANS, DB_USER_IDS)\n",
    "\n",
    "# Later load:\n",
    "# DB_MEANS, DB_USER_IDS = load_embeddings_npz(\"embeddings.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def batch_get_embeddings(paths, embedding_model, size=128, batch_size=16):\n",
    "    embs = []\n",
    "    batch = []\n",
    "    for p in paths:\n",
    "        img = preprocess_signature(img_input=p, size=size, as_tensor=True)\n",
    "        img = np.asarray(img)\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            img = np.transpose(img,(1,2,0))\n",
    "        elif img.ndim == 2:\n",
    "            img = np.expand_dims(img, -1)\n",
    "        batch.append(img.astype(np.float32))\n",
    "        if len(batch) >= batch_size:\n",
    "            X = np.stack(batch, axis=0)\n",
    "            e = embedding_model.predict(X, verbose=0)\n",
    "            embs.append(e)\n",
    "            batch=[]\n",
    "    if batch:\n",
    "        X = np.stack(batch, axis=0)\n",
    "        e = embedding_model.predict(X, verbose=0)\n",
    "        embs.append(e)\n",
    "    if len(embs)==0:\n",
    "        return np.zeros((0, embedding_model.output_shape[-1]), dtype=np.float32)\n",
    "    embs = np.vstack(embs)\n",
    "    embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12)\n",
    "    return embs\n",
    "\n",
    "def compute_eer_threshold(full_org_dir, full_forg_dir, embedding_model, refs_per_user=3, size=128):\n",
    "    # group paths\n",
    "    org_paths = sorted(glob.glob(os.path.join(full_org_dir, \"*.png\")))\n",
    "    forg_paths = sorted(glob.glob(os.path.join(full_forg_dir, \"*.png\")))\n",
    "    org_by_sid = {}\n",
    "    forg_by_sid = {}\n",
    "    for p in org_paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        org_by_sid.setdefault(sid, []).append(p)\n",
    "    for p in forg_paths:\n",
    "        sid = sid_from_filename(p)\n",
    "        forg_by_sid.setdefault(sid, []).append(p)\n",
    "\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    for sid, paths in org_by_sid.items():\n",
    "        if len(paths) < refs_per_user + 1:\n",
    "            continue\n",
    "        refs = paths[:refs_per_user]\n",
    "        ref_embs = batch_get_embeddings(refs, embedding_model, size=size)\n",
    "        ref_mean = np.mean(ref_embs, axis=0)\n",
    "        ref_mean = ref_mean / (np.linalg.norm(ref_mean) + 1e-12)\n",
    "\n",
    "        for test_p in paths[refs_per_user:]:\n",
    "            emb = batch_get_embeddings([test_p], embedding_model, size=size)[0]\n",
    "            score = float(np.dot(emb, ref_mean))\n",
    "            y_scores.append(score); y_true.append(1)\n",
    "        for f in forg_by_sid.get(sid, []):\n",
    "            emb = batch_get_embeddings([f], embedding_model, size=size)[0]\n",
    "            score = float(np.dot(emb, ref_mean))\n",
    "            y_scores.append(score); y_true.append(0)\n",
    "\n",
    "    if len(set(y_true)) < 2:\n",
    "        raise RuntimeError(\"Not enough genuine/forged data to compute EER.\")\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer_thr = thresholds[eer_idx]\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2.0\n",
    "    print(f\"AUC={auc:.4f}, EER={eer:.4f}, EER_threshold={eer_thr:.4f}\")\n",
    "    return eer_thr, {\"auc\":auc, \"eer\":eer}\n",
    "\n",
    "# Example (may take some time on CPU)\n",
    "# thr, metrics = compute_eer_threshold(\"signatures/full_org\", \"signatures/full_forg\", embedding_model, refs_per_user=3, size=128)\n",
    "# print(\"Use threshold:\", thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194116f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_original_or_forged(image_path, embedding_model, db_means, db_user_ids, threshold=0.75, size=128):\n",
    "    query_emb = get_embedding_from_model(image_path, embedding_model, size=size)\n",
    "    sims = db_means @ query_emb   # vectorized cosine (means should be normalized)\n",
    "    best_idx = int(np.argmax(sims))\n",
    "    best_score = float(sims[best_idx])\n",
    "    matched_user = int(db_user_ids[best_idx])\n",
    "    if best_score >= threshold:\n",
    "        return {\"result\":\"ORIGINAL\", \"matched_user_id\": matched_user, \"score\": best_score}\n",
    "    else:\n",
    "        return {\"result\":\"FORGED\", \"matched_user_id\": None, \"score\": best_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f31135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'ORIGINAL', 'matched_user_id': 0, 'score': 0.9703359007835388}\n",
      "{'result': 'FORGED', 'matched_user_id': None, 'score': 0.891384482383728}\n"
     ]
    }
   ],
   "source": [
    "# load saved DB if needed\n",
    "DB_MEANS, DB_USER_IDS = load_embeddings_npz(\"embeddings.npz\")\n",
    "\n",
    "# compute threshold once (recommended)\n",
    "# thr, _ = compute_eer_threshold(\"signatures/full_org\", \"signatures/full_forg\", embedding_model)\n",
    "\n",
    "# quick test\n",
    "res1 = predict_original_or_forged(\"signatures/full_org/original_1_1.png\", embedding_model, DB_MEANS, DB_USER_IDS, threshold=0.91)\n",
    "res2 = predict_original_or_forged(\"signatures/full_forg/forgeries_9_10.png\", embedding_model, DB_MEANS, DB_USER_IDS, threshold=0.90)\n",
    "print(res1)\n",
    "print(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71eb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
